[{"authors":null,"categories":null,"content":"I am a first second year PhD student in Computer Science at the University of Southern California advised by Prof. Robin Jia.\nI am interested in research on natural language processing (NLP) systems that can reason over text and knowledge bases (KBs). I have worked on retrieval for open-domain question answering (ODQA), knowledge base completion (KBC), and semantic parsing for knowledge base QA (KBQA). More recently, I am exploring aspects of robustness and generalization in these systems.\nPreviously, I was a Research Fellow with the wonderful Information Extraction and Synthesis Lab (IESL) at the University of Massachusetts, Amherst.\n","date":1665619200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1665619200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ameyagodbole.github.io/author/ameya-godbole/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ameya-godbole/","section":"authors","summary":"I am a first second year PhD student in Computer Science at the University of Southern California advised by Prof. Robin Jia.\nI am interested in research on natural language processing (NLP) systems that can reason over text and knowledge bases (KBs).","tags":null,"title":"Ameya Godbole","type":"authors"},{"authors":["Ameya Godbole","Robin Jia"],"categories":[],"content":"","date":1665619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665619200,"objectID":"bce0bf038c368bafdaa5ffaa24de8db3","permalink":"https://ameyagodbole.github.io/publication/benchmarking-long-tail-generalization-with-likelihood-splits/","publishdate":"2022-10-13T00:00:00Z","relpermalink":"/publication/benchmarking-long-tail-generalization-with-likelihood-splits/","section":"publication","summary":"In order to reliably process natural language, NLP systems must generalize to the long tail of rare utterances. We propose a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets. We create 'Likelihood splits' where examples that are assigned lower likelihood by a pre-trained language model (LM) are placed in the test set, and more likely examples are in the training set. This simple approach can be customized to construct meaningful train-test splits for a wide range of tasks. Likelihood splits are more challenging than random splits: relative error rates of state-of-the-art models on our splits increase by 59% for semantic parsing on Spider, 77% for natural language inference on SNLI, and 38% for yes/no question answering on BoolQ compared with the corresponding random splits. Moreover, Likelihood splits create fairer benchmarks than adversarial filtering; when the LM used to create the splits is used as the task model, our splits do not adversely penalize the LM.","tags":["OOD","LLM","generalization"],"title":"Benchmarking Long-tail Generalization with Likelihood Splits","type":"publication"},{"authors":["Rajarshi Das","Ameya Godbole","Ankita Naik","Elliot Tower","Manzil Zaheer","Hannaneh Hajishirzi","Robin Jia","Andrew McCallum"],"categories":[],"content":"","date":1658016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658016000,"objectID":"d7817e9a48262e6a3056181ec7fd7ba0","permalink":"https://ameyagodbole.github.io/publication/knowledge-base-question-answering-by-case-based-reasoning-over-subgraphs/","publishdate":"2022-07-17T00:00:00Z","relpermalink":"/publication/knowledge-base-question-answering-by-case-based-reasoning-over-subgraphs/","section":"publication","summary":"Question answering (QA) over knowledge bases (KBs) is challenging because of the diverse, essentially unbounded, types of reasoning patterns needed. However, we hypothesize in a large KB, reasoning patterns required to answer a query type reoccur for various entities in their respective subgraph neighborhoods. Leveraging this structural similarity between local neighborhoods of different subgraphs, we introduce a semiparametric model (CBR-SUBG) with (i) a nonparametric component that for each query, dynamically retrieves other similar *k*-nearest neighbor (KNN) training queries along with query-specific subgraphs and (ii) a parametric component that is trained to identify the (latent) reasoning patterns from the subgraphs of KNN queries and then apply them to the subgraph of the target query. We also propose an adaptive subgraph collection strategy to select a query-specific compact subgraph, allowing us to scale to full Freebase KB containing billions of facts. We show that CBR-SUBG can answer queries requiring subgraph reasoning patterns and performs competitively with the best models on several KBQA benchmarks. Our subgraph collection strategy also produces more compact subgraphs (e.g. 55% reduction in size for WebQSP while increasing answer recall by 4.85%)","tags":["KBQA","CBR","GNN","QA"],"title":"Knowledge Base Question Answering by Case-based Reasoning over Subgraphs","type":"publication"},{"authors":["Rajarshi Das","Manzil Zaheer","Dung Thai","Ameya Godbole","Ethan Perez","Jay Yoon Lee","Lizhen Tan","Lazaros Polymenakos","Andrew McCallum"],"categories":[],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"0fe9abfa27ffadffe6aa97dcde3f5173","permalink":"https://ameyagodbole.github.io/publication/case-based-reasoning-for-natural-language-queries-over-knowledge-bases/","publishdate":"2020-11-02T16:04:42-05:00","relpermalink":"/publication/case-based-reasoning-for-natural-language-queries-over-knowledge-bases/","section":"publication","summary":"It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions — a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11% on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.","tags":["KBQA","CBR","QA"],"title":"Case-based Reasoning for Natural Language Queries over Knowledge Bases","type":"publication"},{"authors":["Rajarshi Das","Ameya Godbole","Nicholas Monath","Manzil Zaheer","Andrew McCallum"],"categories":[],"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"b71d852b7cfbf8d1affc7c91ee31b73a","permalink":"https://ameyagodbole.github.io/publication/probabilistic-case-based-reasoning-for-open-world-knowledge-graph-completion/","publishdate":"2020-11-02T16:04:42-05:00","relpermalink":"/publication/probabilistic-case-based-reasoning-for-open-world-knowledge-graph-completion/","section":"publication","summary":"A case-based reasoning (CBR) system solves a new problem by retrieving `cases' that are similar to the given problem. If such a system can achieve high accuracy, it is appealing owing to its simplicity, interpretability, and scalability. In this paper, we demonstrate that such a system is achievable for reasoning in knowledge-bases (KBs). Our approach predicts attributes for an entity by gathering reasoning paths from similar entities in the KB. Our probabilistic model estimates the likelihood that a path is effective at answering a query about the given entity. The parameters of our model can be efficiently computed using simple path statistics and require no iterative optimization. Our model is non-parametric, growing dynamically as new entities and relations are added to the KB. On several benchmark datasets our approach significantly outperforms other rule learning approaches and performs comparably to state-of-the-art embedding-based approaches. Furthermore, we demonstrate the effectiveness of our model in an 'open-world' setting where new entities arrive in an online fashion, significantly outperforming state-of-the-art approaches and nearly matching the best offline method.","tags":["KBC","CBR"],"title":"Probabilistic Case-based Reasoning for Open-World Knowledge Graph Completion","type":"publication"},{"authors":[],"categories":[],"content":"Mentor: Dr. Andrew McCallum (CICS, UMass Amherst)\nCollaborator: Rajarshi Das, Nicholas Monath, Manzil Zaheer\nA knowledge graph (KG) is formed by the entities as nodes and relations between entities as directed edges. KG completion is the problem of answering queries of the type (e, r, ?) i.e. given a query entity e and query relation r, find the entity e_ans that has the relation r with e. As you can imagine, this task is used to complete partial KGs by guessing missing edges.\nGiven a new problem, a Case-based Reasoning system:\n Retrieves similar cases seem in the past Reuses solutions to previously solved cases Revises solutions to apply to the current query Retains modified solutions as new cases  We adapt this system for knowledge graph completion by using the training graph as the store of solved cases. Similar entities are found by nearest neighbors using a simple one-hot representation based on connected relations (this captures the notion of entity types). Solutions are paths (sequence of relations) that reach the answer node. Then starting from the query entity, we traverse the graph using the paths collected. If such a system can achieve high accuracy, it is appealing owing to its simplicity, interpretability, and scalability.\nWe demonstrated that this simple system is highly performant in A Simple Approach to Case-Based Reasoning in Knowledge Bases.\nAfter performing an error analysis on this system, we then imposed a probabilistic structure to allow the system to weight the answers. Concretely, we use the prior probability of a path reaching the tail entity for the given query relation and the precision of that path type to downweight spurious paths. We use clustering to help aggregate probabilitities. We demonstrated in Probabilistic Case-based Reasoning for Open-World Knowledge Graph Completion that this systems matches/outperforms baselines on FB122, WN18RR and Nell-995. Moreover, we demonstrated that this method can be applied to dynamic growing KGs with minimal overhead whereas the best embedding based methods deteriorate with successive KG updates.\nWe further entended this strategy to Case-based Reasoning for Natural Language Queries over Knowledge Bases by relying on the strengths of pre-trained generative language models. In this setting, we are expected to generate a SPARQL program given a natural language question. We utilized cases as additional input when conditionally generating the output program (this is reminiscent of prompting and in-context learning that has gained attention in recent years).\nThe previous approach requires supervision in the form of output SPARQL programs. In Knowledge Base Question Answering by Case-based Reasoning over Subgraphs, we further relax this assumption; we diesn a system to answer natural language questions when only answer annotations are available. We designed a contrastive learning based training scheme for graph neural networks (GNN). The answer nodes are found by matching analogous nodes between query and case subgraphs.\n","date":1602979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602979200,"objectID":"832ab6457a99560ddb551650a90ccc3c","permalink":"https://ameyagodbole.github.io/project/case-based-reasoning/","publishdate":"2020-10-18T00:00:00Z","relpermalink":"/project/case-based-reasoning/","section":"project","summary":"Applied CBR to knowledge graph completition with competitive results. Demonstrated usage instances where it outperforms existing embedding-based methods. Working to extend functionality to raw text.","tags":[],"title":"Case-Based Reasoning","type":"project"},{"authors":["Rajarshi Das","Ameya Godbole","Shehzaad Dhuliawala","Manzil Zaheer","Andrew McCallum"],"categories":[],"content":"","date":1592956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592956800,"objectID":"e451b7e0c868e767330b26962942a56f","permalink":"https://ameyagodbole.github.io/publication/a-simple-approach-to-case-based-reasoning-in-knowledge-bases/","publishdate":"2020-11-02T14:13:23-05:00","relpermalink":"/publication/a-simple-approach-to-case-based-reasoning-in-knowledge-bases/","section":"publication","summary":"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires *no training*, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Consider the task of finding a target entity given a source entity and a binary relation. Our approach finds multiple *graph path patterns* that connect similar source entities through the given relation, and looks for pattern matches starting from the query source. Using our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122. We also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches.","tags":["KBC","CBR"],"title":"A Simple Approach to Case-Based Reasoning in Knowledge Bases","type":"publication"},{"authors":[],"categories":[],"content":"Mentor: Dr. Mohammad Hajiesmaili (CICS, UMass Amherst), Dr. Philip Thomas (CICS, UMass AmherstCICS, UMass Amherst)\nThis project deals with the problem of online inventory management with demand constraints. In particular, given a time-varying pricing and demand function for electricity and the ability to store a limited amount of electricity for future use (inventory), the task is to minimize the cost of buying electricity while meeting the demand at every time instant.\nThe existing state-of-the-art techniques [1] propose a behavior rule that is analytically shown to have the lowest possible worst case cost ratio. The authors have shown that the method also works well in the average case against several competitive baselines. However, this rule based method does not learn or adapt to the distributions of price and demand. This project attempted to develop methods using Reinforcement Learning to see whether there is room for improvement by learning from interactions with this environment.\nAfter setting up the RL problem with the price and demand variations as the environment and the battery control as the agent, I applied a range of standard RL algorithms as well as imitation learning (DAGGER [2]). The oracle for imitation learning was the optimal offline solution.\nOutcomes: The RL methods are difficult to train for this problem and even with hyperparameter tuning, the best performing agents cannot beat the SotA baseline. The imitation learning agent fares better than the RL agents ans is competitive with the SotA (but not better).\nI ended the project by outlining avenues for future work including dynamically changing action spaces and two-stage control. Although I am no longer working on this problem, this project is being actively worked on by Prof. Hajiesmaili.\n[1]: Online Inventory Management with Application to Energy Procurement in Data Centers [2]: A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning\n","date":1587254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587254400,"objectID":"c558f8664b7f3261428e6379a35e17b6","permalink":"https://ameyagodbole.github.io/project/data-center-energy-portfolio-optimization/","publishdate":"2020-04-19T00:00:00Z","relpermalink":"/project/data-center-energy-portfolio-optimization/","section":"project","summary":"Proposed, implemented and tested reinforcement learning and imitation learning based agents for data center battery controllers. Outlined future work directions.","tags":["RL","optimization"],"title":"Data Center Energy Portfolio Optimization","type":"project"},{"authors":["Ameya Godbole","Rajarshi Das","Manzil Zaheer","Shehzaad Dhuliawala","Andrew McCallum"],"categories":[],"content":"","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572825600,"objectID":"45336d735666287852af2a917bcc4e22","permalink":"https://ameyagodbole.github.io/publication/reasoning-over-chains-of-facts-for-explainable-multi-hop-inference/","publishdate":"2020-11-02T14:01:44-05:00","relpermalink":"/publication/reasoning-over-chains-of-facts-for-explainable-multi-hop-inference/","section":"publication","summary":"This paper describes our submission to the shared task on “Multi-hop Inference Explanation Regeneration” in TextGraphs workshop at EMNLP 2019 (Jansen and Ustalov, 2019). Our system identifies chains of facts relevant to explain an answer to an elementary science examination question. To counter the problem of ‘spurious chains’ leading to ‘semantic drifts’, we train a ranker that uses contextualized representation of facts to score its relevance for explaining an answer to a question. Our system was ranked first w.r.t the mean average precision (MAP) metric outperforming the second best system by 14.95 points.","tags":["QA"],"title":"Chains-of-Reasoning at TextGraphs 2019 Shared Task: Reasoning over Chains of Facts for Explainable Multi-hop Inference","type":"publication"},{"authors":["Ameya Godbole","Dilip Kavarthapu","Rajarshi Das","Zhiyu Gong","Abhishek Singhal","Mo Yu","Xiaoxiao Guo","Tian Gao","Hamed Zamani","Manzil Zaheer","Andrew McCallum"],"categories":[],"content":"","date":1572825600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572825600,"objectID":"e3c059184e907d5d8966785289dccace","permalink":"https://ameyagodbole.github.io/publication/multi-step-entity-centric-information-retrieval-for-multi-hop-question-answering/","publishdate":"2020-11-02T13:46:24-05:00","relpermalink":"/publication/multi-step-entity-centric-information-retrieval-for-multi-hop-question-answering/","section":"publication","summary":"Multi-hop question answering (QA) requires an information retrieval (IR) system that can find multiple supporting evidence needed to answer the question, making the retrieval process very challenging. This paper introduces an IR technique that uses information of entities present in the initially retrieved evidence to learn to ‘hop’ to other relevant evidence. In a setting, with more than 5 million Wikipedia paragraphs, our approach leads to significant boost in retrieval performance. The retrieved evidence also increased the performance of an existing QA model (without any training) on the benchmark by 10.59 F1.","tags":["ODQA","IR","QA"],"title":"Multi-Step Entity-Centric Information Retrieval for Multi-Hop Question Answering","type":"publication"},{"authors":["Ameya Godbole","Aman Dalmia"],"categories":[],"content":"When discussing deep learning, two models have become the leading buzzwords — Convolutional Neural Networks, which are the topic of this post, and Recurrent Neural Networks, which will be discussed soon.\nThe blog post\nThe venture: Inveterate Learner\n","date":1530255600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530255600,"objectID":"81b65365e3ee6a79b4e59f077159c665","permalink":"https://ameyagodbole.github.io/post/dl_book_chap9/","publishdate":"2018-06-29T00:00:00-07:00","relpermalink":"/post/dl_book_chap9/","section":"post","summary":"This is going to be a series of blog posts on the *Deep Learning book* where we are attempting to provide a summary of each chapter highlighting the concepts that we found to be the most important so that other people can use it as a starting point for reading the chapters, while adding further explanations on few areas that we found difficult to grasp. Read the [post](https://medium.com/inveterate-learner/deep-learning-book-chapter-9-convolutional-networks-45e43bfc718d) on Medium","tags":["dl-book","CNN"],"title":"Deep Learning Book: Chapter 9 — Convolutional Networks","type":"post"},{"authors":["Ameya Godbole","Spoorthy Bhat","Prithwijit Guha"],"categories":null,"content":"","date":1519804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519804800,"objectID":"70d47fe714911464757974421bd164c3","permalink":"https://ameyagodbole.github.io/publication/progressively_balanced_multiclass_neural_trees/","publishdate":"2018-02-28T00:00:00-08:00","relpermalink":"/publication/progressively_balanced_multiclass_neural_trees/","section":"publication","summary":"Decision trees are discriminative classifiers that hierarchically partition the input space to achieve regions containing instances having uniform class label. Existing works in this area have mostly focused on C4.5 trees that learn axis aligned partitions. On the other hand, neural trees learn oblique partitions from data and use lesser number of decision nodes hosting perceptrons. However, these perceptrons are susceptible to data imbalances. This motivated us to propose a progressively balanced neural tree where training dataset are balanced prior to perceptron learning. The second contribution is the optimization of the decision function with respect to entropy impurity based objective functions. This formulation also allows a parent node to have more than two child nodes. The proposed algorithm is benchmarked on ten standard datasets against three baseline multi-class classification algorithms.","tags":["decision trees","ML","classification"],"title":"Progressively Balanced Multi-class Neural Trees","type":"publication"},{"authors":["Ameya Godbole","Aman Dalmia","Sunil Kumar Sahu"],"categories":null,"content":"","date":1517126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517126400,"objectID":"c18a00055f73a8b761cf84e34586b3f1","permalink":"https://ameyagodbole.github.io/publication/quora_question_pairs/","publishdate":"2018-01-28T00:00:00-08:00","relpermalink":"/publication/quora_question_pairs/","section":"publication","summary":"Determining whether two given questions are semantically similar is a fairly challenging task given the different structures and forms that the questions can take. In this paper, we use Gated Recurrent Units(GRU) in combination with other highly used machine learning algorithms like Random Forest, Adaboost and SVM for the similarity prediction task on a dataset released by Quora, consisting of about 400k labeled question pairs. We got the best result by using the Siamese adaptation of a Bidirectional GRU with a Random Forest classifier, which landed us among the top 24% in the competition Quora Question Pairs hosted on Kaggle.","tags":["NLP","ensemble","GRU","RNN"],"title":"Siamese Neural Networks with Random Forest for detecting duplicate question pairs","type":"publication"},{"authors":null,"categories":null,"content":"Mentor: Dr. Prithwijit Guha (Dept. of EEE, IIT Guwahati)\nCollaborator: Spoorthy Bhat\nThis project was my Bachelor’s thesis at IIT Guwahati.\nThe basic idea is to allow each node to learn oblique partitions in the input space as opposed to the axis-aligned partitions learned by a standard C4.5 tree. This required the development of a differentiable criterion.\nDuring the course of the project we realized that this hierarchical partitioning results in data-thinning (i.e. as we go down the tree, the number of samples reaching a node go on decreasing). This causes the trained nodes to be biased when class distributions become unbalanced. Consequently, we resort to data balancing. We tested different methods of artificially balancing the class distribution in the data at each node.\nOur method gave comparable accuracy to existing classifiers on 10 benchmark datasets. What is the gain? The learned trees require fewer computations at test time (on average over the test set) than a multi-layer perceptron (MLP) of comparable accuracy.\nWe designed the framework for training the decision tree and node parameters, and testing the classifier using the Tensorflow package.\n","date":1501570800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501570800,"objectID":"ec957e36c51ce7376d8c8e708d406f6c","permalink":"https://ameyagodbole.github.io/project/progressively_balanced_multiclass_neural_trees/","publishdate":"2017-08-01T00:00:00-07:00","relpermalink":"/project/progressively_balanced_multiclass_neural_trees/","section":"project","summary":"Proposed and tested an entropy impurity based objective function for incorporating a learnable perceptron into the decision tree framework. Demonstrated that the learned classifier achieves comparable accuracy with fewer test time computations than an MLP","tags":["decision trees","ML","classification","tensorflow","python"],"title":"Progressively Balanced Multi-class Neural Trees","type":"project"},{"authors":null,"categories":null,"content":"Mentor: Dr. Prithwijit Guha (Dept. of EEE, IIT Guwahati)\nThe problem of detecting when a television stream has switched from content to commercials (advertisements) is difficult because of the wide range of presentation formats available. Even trickier is detecting inter-commercial transitions. Once these transitions are accurately predicted, methods can be developed to further identify the product being advertised. This problem is of importance to companies and advertisers both, as it helps provide credibility to the claimed number times a particular commercial was broadcasted.\nBoiled down to the basics: the project aimed at developing a classifier that could accurately predict whether a scene change resulted in the start/end of a commercial.\nThe project is being developed by the Multimedia Analytics Lab (IIT Guwahati). I participated in the project as a student intern. My role involved collecting and cleaning a dataset, structuring the experiments, and implementing baselines.\n","date":1496300400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496300400,"objectID":"3c7fa14d945c809f37587732ec714dbb","permalink":"https://ameyagodbole.github.io/project/commercial_segmentation_in_television_stream/","publishdate":"2017-06-01T00:00:00-07:00","relpermalink":"/project/commercial_segmentation_in_television_stream/","section":"project","summary":"Attempted segmentation of commercials in television stream through audio-visual feature engineering and application of sequence classifier.","tags":["video processing","image processing","ffmpeg"],"title":"Commercial Segmentation in Television Stream","type":"project"},{"authors":null,"categories":null,"content":"","date":1493622000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493622000,"objectID":"9afa1d1b669281ae63d4aac16c099544","permalink":"https://ameyagodbole.github.io/project/quora_question_pairs/","publishdate":"2017-05-01T00:00:00-07:00","relpermalink":"/project/quora_question_pairs/","section":"project","summary":"Trained a Siamese Gated Recurrent Unit (GRU) RNN over sentence pairs to detect duplicate questions, securing a position in the top 25% among 3000+ teams on Kaggle.","tags":["NLP","ensemble","GRU","RNN","keras","python","pandas","python-multiprocessing"],"title":"Quora Question Pairs","type":"project"},{"authors":null,"categories":null,"content":"Mentor: Dr. John Jose (Dept. of CSE, IIT Guwahati)\nCollaborators: Nandan Bedekar, N Hariharan\nOn-chip interconnection networks are gaining popularity due increase in the number of cores on a processor chip. These networks allow multicore processing without the delays introduced by direct wiring. We modified the network traffic simulator BookSim 2.0 to study the properties of bufferless routers. In particular, we encoded the CHIPPER architecture.\nProject repository\n","date":1485936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485936000,"objectID":"2ee2dc2a75ca4dc50f1ead6308c73d21","permalink":"https://ameyagodbole.github.io/project/bufferless_routeing_in_network_on_chip/","publishdate":"2017-02-01T00:00:00-08:00","relpermalink":"/project/bufferless_routeing_in_network_on_chip/","section":"project","summary":"Designed a network traffic simulator to study on-chip interconnection networks composed of bufferless routers.","tags":["networks","OOP","C++"],"title":"Bufferless Routeing in Network-On-Chip","type":"project"},{"authors":null,"categories":null,"content":"The Robotics Club of IIT Guwahati developed this project. The aim was to learn on the go which was facilitated through tutorials by senior members of the club and the institute as a whole. The project resulted in a robot that could:\n Map its environment and create a 3D rendering of the world using a stereo-camera (which provided depth information). We later migrated to the Kinect sensor Localize itself in a known map upon initialization Navigate to a given target location (set in the map)  The software was built on top of the Robot Operating System (ROS) which provide packages for handling all the concurrent processes.\n","date":1427871600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1427871600,"objectID":"453e49d73adf5106dcc9d8a4b7e8e747","permalink":"https://ameyagodbole.github.io/project/autonomous_intelligent_robot/","publishdate":"2015-04-01T00:00:00-07:00","relpermalink":"/project/autonomous_intelligent_robot/","section":"project","summary":"Implemented simultaneous localization and mapping (SLAM) and visual odometry to create a robot capable of mapping and navigating its environment.","tags":["planning","SLAM","ROS"],"title":"Autonomous Intelligent Robot","type":"project"}]